"""
train.py ‚Äî Fine-tune BERT (bert-base-uncased) on SST-2 sentiment classification.

‚úÖ Requirements satisfied:
1. Data Preparation via Hugging Face datasets & tokenizer
2. Model Fine-Tuning with Trainer API
3. Evaluation: Accuracy + F1 + classification report + confusion matrix
4. Organized, modular, readable code
5. Deliverables-ready: logs, model saving, optional W&B/TensorBoard
6. Bonus: Early stopping, model checkpointing, gradient clipping, LR scheduler, custom training loop option

Author: Gulrukhsor Akhmadjanova
"""

import os
import argparse
from datetime import datetime
import numpy as np
from datasets import load_dataset
import evaluate
from transformers import (
    BertTokenizerFast,
    BertForSequenceClassification,
    TrainingArguments,
    Trainer,
    EarlyStoppingCallback,
    DataCollatorWithPadding,
    set_seed,
)
from sklearn.metrics import classification_report, confusion_matrix
import torch


# ---------- Compute metrics ----------
def compute_metrics(pred):
    """Compute accuracy and macro F1 using the evaluate library."""
    preds = pred.predictions
    if isinstance(preds, tuple):
        preds = preds[0]
    y_pred = np.argmax(preds, axis=1)
    y_true = pred.label_ids

    accuracy = (y_pred == y_true).mean()
    f1_metric = evaluate.load("f1")
    f1_score = f1_metric.compute(predictions=y_pred, references=y_true, average="macro")["f1"]
    return {"accuracy": float(accuracy), "f1_macro": float(f1_score)}


# ---------- Argument parser ----------
def parse_args():
    parser = argparse.ArgumentParser(description="Fine-tune BERT on SST-2 dataset")
    parser.add_argument("--model_name", type=str, default="bert-base-uncased")
    parser.add_argument("--output_dir", type=str, default="./sst2_outputs")
    parser.add_argument("--epochs", type=int, default=3)
    parser.add_argument("--per_device_train_batch_size", type=int, default=8)
    parser.add_argument("--per_device_eval_batch_size", type=int, default=32)
    parser.add_argument("--lr", type=float, default=2e-5)
    parser.add_argument("--weight_decay", type=float, default=0.01)
    parser.add_argument("--max_length", type=int, default=128)
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--save_total_limit", type=int, default=3)
    parser.add_argument("--logging_steps", type=int, default=200)
    parser.add_argument("--use_custom_loop", action="store_true", help="Run custom PyTorch loop instead of Trainer")
    parser.add_argument("--push_to_hub", action="store_true", help="Push best model to Hugging Face Hub")
    return parser.parse_args()


# ---------- Main training ----------
def main():
    args = parse_args()
    set_seed(args.seed)
    os.makedirs(args.output_dir, exist_ok=True)

    # Load SST-2 dataset
    dataset = load_dataset("glue", "sst2")
    print("üìö Dataset loaded:")
    print({split: len(dataset[split]) for split in dataset.keys()})

    # Load tokenizer and model
    tokenizer = BertTokenizerFast.from_pretrained(args.model_name)
    model = BertForSequenceClassification.from_pretrained(args.model_name, num_labels=2)

    # Preprocessing
    def preprocess(examples):
        return tokenizer(examples["sentence"], truncation=True, padding=False, max_length=args.max_length)

    remove_cols = ["sentence"]
    if "idx" in dataset["train"].column_names:
        remove_cols.append("idx")

    tokenized = dataset.map(preprocess, batched=True, remove_columns=remove_cols)

    # Data collator
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

    # Training arguments
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    training_args = TrainingArguments(
        output_dir=os.path.join(args.output_dir, timestamp),
        evaluation_strategy="epoch",     # fixed key
        save_strategy="epoch",
        learning_rate=args.lr,
        per_device_train_batch_size=args.per_device_train_batch_size,
        per_device_eval_batch_size=args.per_device_eval_batch_size,
        num_train_epochs=args.epochs,
        weight_decay=args.weight_decay,
        load_best_model_at_end=True,
        metric_for_best_model="f1_macro",
        greater_is_better=True,
        save_total_limit=args.save_total_limit,
        fp16=torch.cuda.is_available(),
        logging_dir=os.path.join(args.output_dir, "logs"),
        logging_strategy="steps",
        logging_steps=args.logging_steps,
        report_to=["tensorboard"],
        run_name=f"sst2-bert-{timestamp}",
    )

    # Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized["train"],
        eval_dataset=tokenized["validation"],
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],
    )

    # Train model
    if args.use_custom_loop:
        custom_train_loop(model, tokenized["train"], tokenized["validation"], tokenizer, args)
    else:
        trainer.train()
        metrics = trainer.evaluate()
        print("\nüìä Validation Metrics:", metrics)

        # Save best model
        best_dir = os.path.join(training_args.output_dir, "best_model")
        trainer.save_model(best_dir)
        tokenizer.save_pretrained(best_dir)
        print(f"‚úÖ Model saved at: {best_dir}")

        # Detailed evaluation
        preds_output = trainer.predict(tokenized["validation"])
        preds = preds_output.predictions
        if isinstance(preds, tuple):
            preds = preds[0]
        y_pred = np.argmax(preds, axis=1)
        y_true = preds_output.label_ids

        print("\nClassification Report:")
        print(classification_report(y_true, y_pred, digits=4))
        print("\nConfusion Matrix:")
        print(confusion_matrix(y_true, y_pred))

    if args.push_to_hub:
        trainer.push_to_hub()


# ---------- Optional Custom Loop ----------
def custom_train_loop(model, train_dataset, val_dataset, tokenizer, args):
    """Custom training loop for bonus points (manual PyTorch training)."""
    from torch.utils.data import DataLoader
    from transformers import get_linear_schedule_with_warmup
    from torch.optim import AdamW

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
    train_loader = DataLoader(train_dataset, batch_size=args.per_device_train_batch_size, shuffle=True, collate_fn=data_collator)
    val_loader = DataLoader(val_dataset, batch_size=args.per_device_eval_batch_size, collate_fn=data_collator)

    optimizer = AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    total_steps = len(train_loader) * args.epochs
    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.06 * total_steps), num_training_steps=total_steps)

    best_f1 = 0.0
    patience_counter = 0
    early_stop_patience = 2

    f1_metric = evaluate.load("f1")

    for epoch in range(args.epochs):
        model.train()
        total_loss = 0.0
        for batch in train_loader:
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**batch)
            loss = outputs.loss
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            scheduler.step()
            optimizer.zero_grad()
            total_loss += loss.item()
        avg_train_loss = total_loss / len(train_loader)

        # Validation
        model.eval()
        all_preds, all_labels = [], []
        for batch in val_loader:
            labels = batch["labels"]
            batch = {k: v.to(device) for k, v in batch.items()}
            with torch.no_grad():
                logits = model(**batch).logits
            all_preds.append(logits.cpu().numpy())
            all_labels.append(labels.numpy())
        all_preds = np.concatenate(all_preds)
        all_labels = np.concatenate(all_labels)
        y_pred = np.argmax(all_preds, axis=1)
        acc = (y_pred == all_labels).mean()
        f1_score = f1_metric.compute(predictions=y_pred, references=all_labels, average="macro")["f1"]

        print(f"Epoch {epoch+1}/{args.epochs} | Train loss: {avg_train_loss:.4f} | Val acc: {acc:.4f} | Val f1: {f1_score:.4f}")

        # Early stopping
        if f1_score > best_f1:
            best_f1 = f1_score
            patience_counter = 0
            save_dir = os.path.join(args.output_dir, "custom_best")
            os.makedirs(save_dir, exist_ok=True)
            model.save_pretrained(save_dir)
            tokenizer.save_pretrained(save_dir)
            print(f"‚úÖ New best F1: {best_f1:.4f} -> Model saved to {save_dir}")
        else:
            patience_counter += 1
            if patience_counter >= early_stop_patience:
                print("‚èπÔ∏è Early stopping triggered.")
                break


if __name__ == "__main__":
    main()
