# ğŸ§  Fine-Tuning BERT on Sentiment Analysis (SST-2)

> **Author:** Gulrukhsor Akhmadjanova  
> **Task:** Machine Learning Engineer Assignment â€” Fine-Tuning BERT (`bert-base-uncased`)  
> **Goal:** Predict sentiment (positive / negative) on the SST-2 dataset using Hugging Face Transformers.

---

## ğŸ“Œ Overview

This project fine-tunes a **pretrained BERT model** on the **Stanford Sentiment Treebank (SST-2)** dataset â€” a benchmark for sentiment classification.  
The model learns to classify sentences into **positive** or **negative** sentiment categories.  
All training and evaluation steps follow modern NLP standards using the **ğŸ¤— Transformers**, **Datasets**, and **Evaluate** libraries.

---

## ğŸ—‚ï¸ Project Structure


---

## ğŸ“Š Dataset: SST-2

| Property | Description |
|-----------|--------------|
| **Dataset Name** | GLUE â€” SST-2 (Stanford Sentiment Treebank) |
| **Task Type** | Binary Sentiment Classification |
| **Labels** | `0` â†’ Negative, `1` â†’ Positive |
| **Train Size** | ~67,000 samples |
| **Validation Size** | ~1,800 samples |
| **Source** | [ğŸ¤— Hugging Face Datasets: glue/sst2](https://huggingface.co/datasets/glue/viewer/sst2) |

---

## âš™ï¸ Setup & Installation

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/<your_username>/bert-sst2.git
cd bert-sst2

