# TASK 1:  Fine-Tuning BERT on Sentiment Analysis (SST-2)

> **Author:** Gulrukhsor Akhmadjanova
> **Task:** Machine Learning Engineer Assignment â€” Fine-Tuning BERT (`bert-base-uncased`)
> **Goal:** Predict sentiment (positive / negative) on the SST-2 dataset using Hugging Face Transformers.

---

## ğŸ“Œ Overview

This project fine-tunes a **pretrained BERT model** on the **Stanford Sentiment Treebank (SST-2)** dataset â€” a benchmark for sentiment classification.
The model learns to classify sentences into **positive** or **negative** sentiment categories.
All training and evaluation steps follow modern NLP standards using the **ğŸ¤— Transformers**, **Datasets**, and **Evaluate** libraries.

---

## ğŸ—‚ï¸ Project Structure

```
train.py                 # Main training script (GitHub)
requirements.txt         # Python dependencies
README.md                # Project documentation
```

> **Note:** Evaluation is performed entirely in **Google Colab** (`task1_uzcosmos.ipynb`). All outputs (checkpoints, metrics, logs) are saved in Colab.

---

## ğŸ“Š Dataset: SST-2

| Property            | Description                                                                             |
| ------------------- | --------------------------------------------------------------------------------------- |
| **Dataset Name**    | GLUE â€” SST-2 (Stanford Sentiment Treebank)                                              |
| **Task Type**       | Binary Sentiment Classification                                                         |
| **Labels**          | `0` â†’ Negative, `1` â†’ Positive                                                          |
| **Train Size**      | ~67,000 samples                                                                         |
| **Validation Size** | ~1,800 samples                                                                          |
| **Source**          | [ğŸ¤— Hugging Face Datasets: glue/sst2](https://huggingface.co/datasets/glue/viewer/sst2) |

---

## âš™ï¸ Setup & Installation

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/<your_username>/bert-sst2.git
cd bert-sst2
```

### 2ï¸âƒ£ Install Dependencies

```bash
pip install --upgrade pip
pip install transformers datasets evaluate scikit-learn sentencepiece wandb tensorboard
pip install -q accelerate  # optional for faster training
```

Or install all packages from `requirements.txt`:

```bash
pip install -r requirements.txt
```

**requirements.txt content:**

```
transformers
datasets
evaluate
scikit-learn
sentencepiece
wandb
tensorboard
accelerate
torch
numpy
matplotlib
tqdm
```

---

## ğŸ‹ï¸ Training

The training script (`train.py`) supports:

* Hugging Face **Trainer API** (default)
* Optional **custom PyTorch training loop** (manual optimization)

### Standard Training Command

```bash
python train.py --output_dir ./sst2_outputs --epochs 3 \
--per_device_train_batch_size 16 --per_device_eval_batch_size 64 --lr 2e-5
```

> âš ï¸ On Colab Free GPU, reduce `per_device_train_batch_size` to 8 or 4 if you run out of memory.

### Optional Custom Loop

```bash
python train.py --use_custom_loop
```

> Note: Using `--use_custom_loop` **restarts training** from scratch.

All checkpoints are saved in `sst2_outputs/` in Google Colab.

---

## ğŸ“Š Evaluation in Google Colab

Evaluation runs **entirely in Colab** via `task1_uzcosmos.ipynb` and includes:

* Loading the latest checkpoint from `sst2_outputs/`
* Computing **Accuracy** and **F1-score**
* Generating **classification report** and **confusion matrix**
* Saving metrics to `results/eval_metrics.npz`

> No evaluation scripts are included in GitHub; all evaluation outputs live in Colab.

---

## ğŸ“ˆ Features

* Automatic detection of latest model checkpoint
* Accuracy & F1-score evaluation
* Classification report & confusion matrix visualization
* Optional custom training loop
* Metrics saved for further analysis

---

## ğŸ“Œ Notes

* All model outputs and evaluation metrics are saved **inside Google Colab**.
* To reproduce results locally, download the outputs from Colab.
* GitHub repo contains **training scripts and dependencies only**.

---

## ğŸ“– References

* **BERT**: [https://huggingface.co/bert-base-uncased](https://huggingface.co/bert-base-uncased)
* **SST-2 Dataset (GLUE)**: [https://huggingface.co/datasets/glue](https://huggingface.co/datasets/glue)

# TASK 2: ğŸ›°ï¸ Road Detection from Aerial Images

This project focuses on **binary semantic segmentation** to detect roads from high-resolution aerial or satellite images.  
The goal is to identify pixels belonging to roads (`1`) and background (`0`) using deep learning.

---

## ğŸ¯ Objective
Build and train a model capable of automatically segmenting roads from aerial imagery using architectures like **U-Net**.  
This project demonstrates understanding of **image segmentation**, **data preprocessing**, **model development**, **evaluation**, and **post-processing**.

---

## ğŸ“š Dataset
**Used:** Synthetic/Fake dataset generated within the Colab notebook for demonstration.  
**Not yet implemented:** Real datasets such as **Massachusetts Roads** or **DeepGlobe Road Extraction**.

Each generated image has a corresponding ground truth mask of the same resolution.

---

## âš™ï¸ Project Structure
- Implemented in **Google Colab**
- GitHub repository serves as a **reference link** to Colab code
- All experiments, visualization, and training are done in Colab

---

## âœ… Completed Requirements

### ğŸ§© Data Preprocessing
- [x] Dataset loading (synthetic dataset created in code)
- [x] Data inspection (visualized random images and masks)
- [x] Resizing (handled during dataset creation)
- [x] Normalization (image tensors normalized to `[0, 1]`)
- [x] Dataset class implemented (`RoadDataset`)
- [ ] Real dataset loading (Massachusetts/DeepGlobe)
- [ ] Advanced augmentation (only basic random noise/lines used)

---

### ğŸ§  Model Architecture
- [x] Implemented **U-Net** architecture from scratch
- [x] Supports modular layers for encoderâ€“decoder
- [ ] No pretrained backbone (e.g., ResNet/EfficientNet) yet

---

### ğŸ‹ï¸â€â™€ï¸ Training
- [x] Training loop implemented
- [x] Loss function: Binary Cross Entropy (BCE)
- [x] IoU metric implemented
- [x] Dice metric implemented
- [x] Tracks loss and IoU over epochs
- [ ] Train/validation split (currently uses full dataset)
- [ ] Combined losses (Dice + BCE not yet used)

---

### ğŸ“Š Evaluation
- [x] Test set evaluation (IoU and Dice)
- [x] Visualization: input, ground truth, prediction (side-by-side)
- [x] Plots: training loss and IoU curves
- [ ] Evaluation on real dataset

---

### ğŸ§± Code Quality & Reproducibility
- [x] Clear, modular code (dataset class, model class, metrics, etc.)
- [x] Configurable hyperparameters (batch size, epochs, learning rate)
- [x] Requirements saved to `requirements.txt`
- [x] Model saved as `road_detection_model.pth`
- [x] Training information saved as `.json`
- [x] Summary printed after training
- [x] All visualizations automated
- [x] Works fully in Google Colab
- [ ] External dataset configuration not yet added

---

### ğŸ… Bonus (Optional)
- [x] Post-processing with morphological operations (OpenCV)
- [x] Mask-to-vector polygon conversion (Shapely)
- [x] Overlay predictions on image
- [x] Pretrained model option (U-Net with ResNet34 backbone)
- [ ] Tile-based training for large images not implemented

---

## ğŸ§¾ Summary

| Category | Description | Status |
|-----------|-------------|--------|
| **Data Preprocessing** | Synthetic data + normalization | âœ… Partial |
| **Model** | U-Net (custom) | âœ… Done |
| **Training** | BCE + IoU + Dice metrics | âœ… Partial |
| **Evaluation** | Visualization & plots | âœ… Done |
| **Post-processing** | Morphology + Polygon extraction | âœ… Done |
| **Dataset** | Real dataset integration | âŒ Not yet |
| **Tile-based training** | Not implemented | âŒ Not yet |

---

## ğŸ’» Technologies Used
- Python 3.x  
- PyTorch  
- OpenCV  
- NumPy, Matplotlib  
- Shapely  
- tqdm  

---

## ğŸš€ How to Run
1. Open the Colab notebook link.
2. Run all cells in order.
3. Model will train, evaluate, and visualize predictions.
4. Outputs:
   - Model weights (`road_detection_model.pth`)
   - Metrics & training logs (`training_info.json`)
   - Plots & predicted masks

---

## ğŸ“ˆ Future Improvements
- Integrate real aerial datasets (Massachusetts/DeepGlobe)
- Add advanced augmentations (flip, rotate, crop)
- Implement hybrid losses (BCE + Dice)
- Add validation split and early stopping
- Support tile-based training for large images

---

## ğŸ‘©â€ğŸ’» Author
**Gulrukhsor Akhmadjanova**  
Google Colab Implementation, 2025  
